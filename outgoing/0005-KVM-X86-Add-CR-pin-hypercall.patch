From 0ba81d7aff159fb80e21b8a53a89b2c4eb518593 Mon Sep 17 00:00:00 2001
From: John Andersen <john.s.andersen@intel.com>
Date: Tue, 12 Nov 2019 17:28:04 -0500
Subject: [PATCH 5/6] KVM: X86: Add CR pin hypercall

Add a CR pin feature bit to the KVM cpuid. Add a read only MSR to KVM
which guests use to identify which bits they may request be pinned. Add
CR pin hypercall to KVM. Allow guests to request that KVM pin certain
bits within control register 0 or 4. If bits have already been pinned,
or the guest attempts to pin unsupported bits, return -KVM_EINVAL.
In the event that the guest vcpu attempts to disable any of the pinned
bits, send that vcpu a general protection fault, and leave the register
unchanged. Disregard pinning when running in SMM. Entering SMM disables
pinned bits, writes to control registers within SMM would therefore
trigger general protection faults if pinning was enforced. When syncing
control registers with userspace if userspace clears bits that are
pinned, unpin those bits.

KVM has no concept of reboot, QEMU zeros control registers on reboot.
If pining bits are not cleared when userspace clears their control
register counterparts, initial writes to control registers by the
rebooted guest cause general protection faults.

Pinning of sensitive CR bits has already been implemented to protect
against exploits directly calling native_write_cr*(). The current
protection cannot stop ROP attacks which jump directly to a MOV CR
instruction.

https://web.archive.org/web/20171029060939/http://www.blackbunny.io/linux-kernel-x86-64-bypass-smep-kaslr-kptr_restric/

Guests running with hypervisor based CR pinning are now protected
against the use of ROP to disable CR bits. The same bits that are being
pinned in the current implementation can be pinned via the hypervisor.
These bits are WP in CR0, and SMEP, SMAP, and UMIP in CR4.

Other hypervisors such as HyperV have implemented similar protections
for Control Registers and MSRs; which security researchers have found
effective.

https://www.abatchy.com/2018/01/kernel-exploitation-4

Future patches could implement similar MSR and hypercall combinations
to protect bits in MSRs. The NXE bit of the EFER MSR is a prime
candidate.

A patch for QEMU is required to expose the CR pin cpuid feature bit.

https://github.com/qemu/qemu/commit/b9c6e4f3c12e14c9a1d715fa1b8a9a2035bf7d04

The usage of SMM in SeaBIOS was explored as a way to communicate to KVM
that a reboot has occurred and it should zero the pinned bits. When
using QEMU and SeaBIOS, SMM initialization occurs on reboot. However,
prior to SMM initialization, BIOS writes zero values to CR0, causing a
general protection fault to be sent to the guest before SMM can signal
that the machine has booted.

Signed-off-by: John Andersen <john.s.andersen@intel.com>
---
 Documentation/virt/kvm/hypercalls.txt | 21 +++++++++
 Documentation/virt/kvm/msr.txt        | 12 +++++
 arch/x86/include/asm/kvm_host.h       |  2 +
 arch/x86/include/uapi/asm/kvm_para.h  |  3 ++
 arch/x86/kvm/cpuid.c                  |  3 +-
 arch/x86/kvm/x86.c                    | 68 +++++++++++++++++++++++++++
 include/uapi/linux/kvm_para.h         |  1 +
 7 files changed, 109 insertions(+), 1 deletion(-)

diff --git a/Documentation/virt/kvm/hypercalls.txt b/Documentation/virt/kvm/hypercalls.txt
index 5f6d291bd004..b17dcef6aec6 100644
--- a/Documentation/virt/kvm/hypercalls.txt
+++ b/Documentation/virt/kvm/hypercalls.txt
@@ -152,3 +152,24 @@ a0: destination APIC ID
 
 Usage example: When sending a call-function IPI-many to vCPUs, yield if
 any of the IPI target vCPUs was preempted.
+
+12. KVM_HC_CR_PIN
+------------------------
+Architecture: x86
+Status: active
+Purpose: Hypercall used to configure pinned bits in control registers
+Usage:
+
+a0: Control register index
+
+a1: Mask of bits which should be 0 or 1 in the control register.
+
+The hypercall lets a guest enable bit pinning for control registers.
+Instructing the host to enforce guest specified restrictions upon the guest.
+These restrictions cannot be changed for the lifetime of the guest vcpu. This
+is to prevent attackers from disabling them before executing an attack they may
+prevent.
+
+Returns KVM_EINVAL if a1 is 0 or if a1 contains bits which are not allowed to
+be pinned. Bits which are allowed to be pinned can be found by reading the
+MSR_KVM_CR0_PIN_ALLOWED and MSR_KVM_CR4_PIN_ALLOWED MSRs.
diff --git a/Documentation/virt/kvm/msr.txt b/Documentation/virt/kvm/msr.txt
index df1f4338b3ca..649c5890d47e 100644
--- a/Documentation/virt/kvm/msr.txt
+++ b/Documentation/virt/kvm/msr.txt
@@ -282,3 +282,14 @@ MSR_KVM_POLL_CONTROL: 0x4b564d05
 	KVM guests can request the host not to poll on HLT, for example if
 	they are performing polling themselves.
+
+MSR_KVM_CR0_PIN_ALLOWED: 0x4b564d06
+MSR_KVM_CR4_PIN_ALLOWED: 0x4b564d07
+	Read only registers informing the guest which bits may be pinned for
+	each control register respectively via the CR pin hypercall.
+
+	data: Bits which may be pinned. Attempting to pin bits other than these
+	will result in a return code of -EINVAL from the CR pin hypercall.
+
+	Bits which are allowed to be pinned are WP for CR0 and SMEP, SMAP, and
+	UMIP for CR4.
 
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ab8550182a21..6e52f6a2dee9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -545,10 +545,12 @@ struct kvm_vcpu_arch {
 
 	unsigned long cr0;
 	unsigned long cr0_guest_owned_bits;
+	unsigned long cr0_pinned;
 	unsigned long cr2;
 	unsigned long cr3;
 	unsigned long cr4;
 	unsigned long cr4_guest_owned_bits;
+	unsigned long cr4_pinned;
 	unsigned long cr8;
 	u32 pkru;
 	u32 hflags;
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 2a8e0b6b9805..1fcfdb6c499e 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -31,6 +31,7 @@
 #define KVM_FEATURE_PV_SEND_IPI	11
 #define KVM_FEATURE_POLL_CONTROL	12
 #define KVM_FEATURE_PV_SCHED_YIELD	13
+#define KVM_FEATURE_CR_PIN		14
 
 #define KVM_HINTS_REALTIME      0
 
@@ -50,6 +51,8 @@
 #define MSR_KVM_STEAL_TIME  0x4b564d03
 #define MSR_KVM_PV_EOI_EN      0x4b564d04
 #define MSR_KVM_POLL_CONTROL	0x4b564d05
+#define MSR_KVM_CR0_PIN_ALLOWED	0x4b564d06
+#define MSR_KVM_CR4_PIN_ALLOWED	0x4b564d07
 
 struct kvm_steal_time {
 	__u64 steal;
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index f68c0c753c38..37dd3aa20274 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -712,7 +712,8 @@ static inline int __do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 function,
 			     (1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |
 			     (1 << KVM_FEATURE_PV_SEND_IPI) |
 			     (1 << KVM_FEATURE_POLL_CONTROL) |
-			     (1 << KVM_FEATURE_PV_SCHED_YIELD);
+			     (1 << KVM_FEATURE_PV_SCHED_YIELD) |
+			     (1 << KVM_FEATURE_CR_PIN);
 
 		if (sched_info_on())
 			entry->eax |= (1 << KVM_FEATURE_STEAL_TIME);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 5d530521f11d..8a996ec68538 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -767,6 +767,9 @@ int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	if ((cr0 & X86_CR0_PG) && !(cr0 & X86_CR0_PE))
 		return 1;
 
+	if (!is_smm(vcpu) && (cr0 ^ old_cr0) & vcpu->arch.cr0_pinned)
+		return 1;
+
 	if (!is_paging(vcpu) && (cr0 & X86_CR0_PG)) {
 #ifdef CONFIG_X86_64
 		if ((vcpu->arch.efer & EFER_LME)) {
@@ -923,6 +926,9 @@ int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	if (kvm_valid_cr4(vcpu, cr4))
 		return 1;
 
+	if (!is_smm(vcpu) && (cr4 ^ old_cr4) & vcpu->arch.cr4_pinned)
+		return 1;
+
 	if (is_long_mode(vcpu)) {
 		if (!(cr4 & X86_CR4_PAE))
 			return 1;
@@ -1241,6 +1247,8 @@ static const u32 emulated_msrs_all[] = {
 
 	MSR_K7_HWCR,
 	MSR_KVM_POLL_CONTROL,
+	MSR_KVM_CR0_PIN_ALLOWED,
+	MSR_KVM_CR4_PIN_ALLOWED,
 };
 
 static u32 emulated_msrs[ARRAY_SIZE(emulated_msrs_all)];
@@ -2921,6 +2929,9 @@ static int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)
 	return 0;
 }
 
+#define KVM_CR0_PIN_ALLOWED	(X86_CR0_WP)
+#define KVM_CR4_PIN_ALLOWED	(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP)
+
 int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	switch (msr_info->index) {
@@ -3040,6 +3051,12 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	case MSR_KVM_POLL_CONTROL:
 		msr_info->data = vcpu->arch.msr_kvm_poll_control;
 		break;
+	case MSR_KVM_CR0_PIN_ALLOWED:
+		msr_info->data = KVM_CR0_PIN_ALLOWED;
+		break;
+	case MSR_KVM_CR4_PIN_ALLOWED:
+		msr_info->data = KVM_CR4_PIN_ALLOWED;
+		break;
 	case MSR_IA32_P5_MC_ADDR:
 	case MSR_IA32_P5_MC_TYPE:
 	case MSR_IA32_MCG_CAP:
@@ -7374,6 +7391,44 @@ static void kvm_sched_yield(struct kvm *kvm, unsigned long dest_id)
 		kvm_vcpu_yield_to(target);
 }
 
+static unsigned long kvm_pin_cr_bits(struct kvm_vcpu *vcpu,
+				     unsigned long cr,
+				     unsigned long pin)
+{
+	unsigned long ret;
+
+	if (!pin)
+		return -KVM_EINVAL;
+
+	switch (cr) {
+	case 0:
+		if (vcpu->arch.cr0_pinned || (pin & ~KVM_CR0_PIN_ALLOWED)) {
+			ret = -KVM_EINVAL;
+			break;
+		}
+		vcpu->arch.cr0_pinned = pin;
+		kvm_x86_ops->set_cr0_guest_owned_bits(vcpu,
+				vcpu->arch.cr0_guest_owned_bits & ~pin);
+		ret = 0;
+		break;
+	case 4:
+		if (vcpu->arch.cr4_pinned || (pin & ~KVM_CR4_PIN_ALLOWED)) {
+			ret = -KVM_EINVAL;
+			break;
+		}
+		vcpu->arch.cr4_pinned = pin;
+		kvm_x86_ops->set_cr4_guest_owned_bits(vcpu,
+				vcpu->arch.cr4_guest_owned_bits & ~pin);
+		ret = 0;
+		break;
+	default:
+		ret = -KVM_EOPNOTSUPP;
+		break;
+	}
+
+	return ret;
+}
+
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 {
 	unsigned long nr, a0, a1, a2, a3, ret;
@@ -7425,6 +7480,9 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 		kvm_sched_yield(vcpu->kvm, a0);
 		ret = 0;
 		break;
+	case KVM_HC_CR_PIN:
+		ret = kvm_pin_cr_bits(vcpu, a0, a1);
+		break;
 	default:
 		ret = -KVM_ENOSYS;
 		break;
@@ -8802,10 +8860,20 @@ static int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 	mmu_reset_needed |= vcpu->arch.efer != sregs->efer;
 	kvm_x86_ops->set_efer(vcpu, sregs->efer);
 
+	vcpu->arch.cr0_pinned &= sregs->cr0;
+	kvm_x86_ops->set_cr0_guest_owned_bits(vcpu,
+			vcpu->arch.cr0_guest_owned_bits &
+			~vcpu->arch.cr0_pinned);
+
 	mmu_reset_needed |= kvm_read_cr0(vcpu) != sregs->cr0;
 	kvm_x86_ops->set_cr0(vcpu, sregs->cr0);
 	vcpu->arch.cr0 = sregs->cr0;
 
+	vcpu->arch.cr4_pinned &= sregs->cr4;
+	kvm_x86_ops->set_cr4_guest_owned_bits(vcpu,
+			vcpu->arch.cr4_guest_owned_bits &
+			~vcpu->arch.cr4_pinned);
+
 	mmu_reset_needed |= kvm_read_cr4(vcpu) != sregs->cr4;
 	cpuid_update_needed |= ((kvm_read_cr4(vcpu) ^ sregs->cr4) &
 				(X86_CR4_OSXSAVE | X86_CR4_PKE));
diff --git a/include/uapi/linux/kvm_para.h b/include/uapi/linux/kvm_para.h
index 8b86609849b9..20916c70d8f2 100644
--- a/include/uapi/linux/kvm_para.h
+++ b/include/uapi/linux/kvm_para.h
@@ -29,6 +29,7 @@
 #define KVM_HC_CLOCK_PAIRING		9
 #define KVM_HC_SEND_IPI		10
 #define KVM_HC_SCHED_YIELD		11
+#define KVM_HC_CR_PIN			12
 
 /*
  * hypercalls use architecture specific
-- 
2.21.0

