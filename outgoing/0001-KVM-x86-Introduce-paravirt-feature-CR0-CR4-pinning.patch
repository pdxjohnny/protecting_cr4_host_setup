From d41da582751b251702e20c1775ab1e4421474c47 Mon Sep 17 00:00:00 2001
From: John Andersen <john.s.andersen@intel.com>
Date: Tue, 12 Nov 2019 17:28:04 -0500
Subject: [PATCH 1/9] KVM: x86: Introduce paravirt feature CR0/CR4 pinning

Add a CR pin feature bit to the KVM cpuid. Add MSRs to KVM which guests
use to identify which bits they may request be pinned. Allowed bits
default to CR0 WP, CR4 SMEP, SMAP, UMIP, and FSGSBASE. Enable host
userspace to modify which bits are allowed to be pinned. Add CR pinned
MSRs to KVM. Allow guests to request that KVM pin certain bits within
control register 0 or 4 via the CR pinned MSRs. Writes to the MSRs fail
if they include bits which aren't allowed. Host userspace may clear or
modify pinned bits at any time. Once pinned bits are set, the guest may
pin additional allowed bits, but not clear any.

In the event that the guest vCPU attempts to disable any of the pinned
bits, send that vCPU a general protection fault, and leave the register
unchanged. Clear pinning on vCPU reset to avoid faulting non-boot
CPUs when they are disabled and then re-enabled, which is done when
hibernating.

Entering SMM zeros control registers, resulting in the disabling of bits
which may be pinned. Ignore pinning MSRs when running in SMM. On exit
from SMM, check SMRAM to ensure the values of CR0/4 that will be
restored contain the correct values for pinned bits.

On VM-Exit check L1 control register values within VMCS against
pinning MSRs. According to Intel Vol 3 27.7 if VMCS misconfiguration
is discovered trigger a VMX abort with the indicator field set to 3.

According to AMD Vol 2 15.6 if VMCB host state inconsistency is
discovered cause the processor to shutdown. Modifications to
nested_svm_vmexit() to ensure that the processor is shutdown are
non-trivial and the author lacks a platform to test any potential
changes. As such, do not set the KVM_FEATURE_CR_PIN CPUID feature on AMD
processors.

Should userspace expose the CR pinning CPUID feature bit, it must zero
CR pinned MSRs on reboot. If it does not, it runs the risk of having the
guest enable pinning and subsequently cause general protection faults on
next boot due to BIOS/EFI code setting control registers to values
which do not contain the pinned bits. Userspace is responsible for
migrating the contents of the CR* MSRs. If userspace fails to migrate
the MSRs the protection will no longer be active.

Changes for QEMU are required to expose the CR pin cpuid feature bit. As
well as clear the MSRs on reboot and enable migration.

https://github.com/pdxjohnny/qemu/commit/9a542186ca5587cf8cefc235c5d17abada0fbccd
https://github.com/pdxjohnny/qemu/commit/b9bb3bd2ea1e9f4783cbf0bc068be186de311eff

Signed-off-by: John Andersen <john.s.andersen@intel.com>
---
 Documentation/virt/kvm/msr.rst       |  70 ++++++++++++++++
 arch/x86/include/asm/kvm_host.h      |   8 ++
 arch/x86/include/uapi/asm/kvm_para.h |   7 ++
 arch/x86/include/uapi/asm/vmx.h      |   1 +
 arch/x86/kvm/cpuid.c                 |   3 +
 arch/x86/kvm/emulate.c               |   3 +-
 arch/x86/kvm/kvm_emulate.h           |   2 +-
 arch/x86/kvm/vmx/nested.c            |  20 ++++-
 arch/x86/kvm/x86.c                   | 120 ++++++++++++++++++++++++++-
 9 files changed, 228 insertions(+), 6 deletions(-)

diff --git a/Documentation/virt/kvm/msr.rst b/Documentation/virt/kvm/msr.rst
index e37a14c323d2..50411ad6331b 100644
--- a/Documentation/virt/kvm/msr.rst
+++ b/Documentation/virt/kvm/msr.rst
@@ -376,3 +376,73 @@ data:
 	write '1' to bit 0 of the MSR, this causes the host to re-scan its queue
 	and check if there are more notifications pending. The MSR is available
 	if KVM_FEATURE_ASYNC_PF_INT is present in CPUID.
+
+MSR_KVM_CR0_PIN_ALLOWED:
+	0x4b564d08
+MSR_KVM_CR4_PIN_ALLOWED:
+	0x4b564d09
+
+	Registers informing the guest which bits may be pinned for each control
+	register respectively via the CR pinned MSRs. Read only for the guest.
+	Host VMM may modify which bits are allowed by writing to these
+	registers.
+
+data:
+	Bits which may be pinned.
+
+	Attempting to pin bits other than these will result in a failure when
+	writing to the respective CR pinned MSR.
+
+	Bits which are allowed to be pinned default to WP for CR0 and SMEP,
+	SMAP, UMIP, and FSGSBASE for CR4.
+
+	The host VMM may modify the set of allowed bits. However, only the above
+	have been tested to work. Allowing the guest to pin other bits may or
+	may not be compatible with KVM. When the host VMM writes a 0 to one of
+	these registers the register will be reset to it's default value. Host
+	VMMs wishing to disallow pinning should not expose the CPU feature bit.
+
+	The MSR is available if KVM_FEATURE_CR_PIN is present in CPUID.
+
+MSR_KVM_CR0_PINNED_ZERO:
+	0x4b564d0a
+MSR_KVM_CR0_PINNED_ONE:
+	0x4b564d0b
+MSR_KVM_CR4_PINNED_ZERO:
+	0x4b564d0c
+MSR_KVM_CR4_PINNED_ONE:
+	0x4b564d0d
+
+	Used to configure pinned bits in control registers
+
+data:
+	Bits to be pinned.
+
+	Fails if data contains bits which are not allowed to be pinned. Or if
+	attempting to pin bits one that are already pinned zero, or vice versa.
+	Bits which are allowed to be pinned can be found by reading the CR pin
+	allowed MSRs.
+
+	The MSRs are read/write for host userspace, and write-only for the
+	guest.
+
+	Once set to a non-zero value, the guest cannot clear any of the bits
+	that have been pinned. The guest can pin more bits, so long as those
+	bits appear in the allowed MSR, and are not already pinned to the
+	opposite value.
+
+	Host userspace may clear or change pinned bits at any point. Host
+	userspace must clear pinned bits on reboot.
+
+	The MSR enables bit pinning for control registers. Pinning means if the
+	guest attempts to write values to CR0/4 where bits differ from pinned
+	bits, the write will fail and the guest will be sent a general
+	protection fault. Pinning is active when the guest is not in SMM. If an
+	exit from SMM results in pinned bits becoming unpinned as a result of
+	resotration from SMRAM, the guest will exit. If nested virtualization is
+	enabled and KVM detects that the host VMCS (L1) contains saved CR0/4
+	values which have bits set to values other than what they should be per
+	pinning MSRs, a VMX-abort will be sent to the guest and the offending
+	bits in CR0/4 will be set according to pinning MSRs.
+
+	The MSR is available if KVM_FEATURE_CR_PIN is present in CPUID.
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index d44858b69353..1e76836dc893 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -520,6 +520,12 @@ struct kvm_vcpu_hv {
 	cpumask_t tlb_flush;
 };
 
+struct kvm_vcpu_cr_pinning {
+	unsigned long allowed;
+	unsigned long one;
+	unsigned long zero;
+};
+
 struct kvm_vcpu_arch {
 	/*
 	 * rip and regs accesses must go through
@@ -531,11 +537,13 @@ struct kvm_vcpu_arch {
 
 	unsigned long cr0;
 	unsigned long cr0_guest_owned_bits;
+	struct kvm_vcpu_cr_pinning cr0_pinned;
 	unsigned long cr2;
 	unsigned long cr3;
 	unsigned long cr4;
 	unsigned long cr4_guest_owned_bits;
 	unsigned long cr4_guest_rsvd_bits;
+	struct kvm_vcpu_cr_pinning cr4_pinned;
 	unsigned long cr8;
 	u32 host_pkru;
 	u32 pkru;
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 950afebfba88..17264d890aae 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -33,6 +33,7 @@
 #define KVM_FEATURE_PV_SCHED_YIELD	13
 #define KVM_FEATURE_ASYNC_PF_INT	14
 #define KVM_FEATURE_MSI_EXT_DEST_ID	15
+#define KVM_FEATURE_CR_PIN		16
 
 #define KVM_HINTS_REALTIME      0
 
@@ -54,6 +55,12 @@
 #define MSR_KVM_POLL_CONTROL	0x4b564d05
 #define MSR_KVM_ASYNC_PF_INT	0x4b564d06
 #define MSR_KVM_ASYNC_PF_ACK	0x4b564d07
+#define MSR_KVM_CR0_PIN_ALLOWED	0x4b564d08
+#define MSR_KVM_CR4_PIN_ALLOWED	0x4b564d09
+#define MSR_KVM_CR0_PINNED_ZERO	0x4b564d0a
+#define MSR_KVM_CR0_PINNED_ONE	0x4b564d0b
+#define MSR_KVM_CR4_PINNED_ZERO	0x4b564d0c
+#define MSR_KVM_CR4_PINNED_ONE	0x4b564d0d
 
 struct kvm_steal_time {
 	__u64 steal;
diff --git a/arch/x86/include/uapi/asm/vmx.h b/arch/x86/include/uapi/asm/vmx.h
index b8ff9e8ac0d5..386b2f357649 100644
--- a/arch/x86/include/uapi/asm/vmx.h
+++ b/arch/x86/include/uapi/asm/vmx.h
@@ -155,6 +155,7 @@
 
 #define VMX_ABORT_SAVE_GUEST_MSR_FAIL        1
 #define VMX_ABORT_LOAD_HOST_PDPTE_FAIL       2
+#define VMX_ABORT_VMCS_CORRUPTED             3
 #define VMX_ABORT_LOAD_HOST_MSR_FAIL         4
 
 #endif /* _UAPIVMX_H */
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index d50041f570e8..bab7123c264f 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -793,6 +793,9 @@ static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 			     (1 << KVM_FEATURE_PV_SCHED_YIELD) |
 			     (1 << KVM_FEATURE_ASYNC_PF_INT);
 
+		if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD)
+			entry->eax |= (1 << KVM_FEATURE_CR_PIN);
+
 		if (sched_info_on())
 			entry->eax |= (1 << KVM_FEATURE_STEAL_TIME);
 
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 0d917eb70319..4b0a84832e88 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -2697,7 +2697,8 @@ static int em_rsm(struct x86_emulate_ctxt *ctxt)
 		return X86EMUL_UNHANDLEABLE;
 	}
 
-	ctxt->ops->post_leave_smm(ctxt);
+	if (ctxt->ops->post_leave_smm(ctxt))
+		return X86EMUL_UNHANDLEABLE;
 
 	return X86EMUL_CONTINUE;
 }
diff --git a/arch/x86/kvm/kvm_emulate.h b/arch/x86/kvm/kvm_emulate.h
index 43c93ffa76ed..e92dd7605e48 100644
--- a/arch/x86/kvm/kvm_emulate.h
+++ b/arch/x86/kvm/kvm_emulate.h
@@ -232,7 +232,7 @@ struct x86_emulate_ops {
 	void (*set_hflags)(struct x86_emulate_ctxt *ctxt, unsigned hflags);
 	int (*pre_leave_smm)(struct x86_emulate_ctxt *ctxt,
 			     const char *smstate);
-	void (*post_leave_smm)(struct x86_emulate_ctxt *ctxt);
+	int (*post_leave_smm)(struct x86_emulate_ctxt *ctxt);
 	int (*set_xcr)(struct x86_emulate_ctxt *ctxt, u32 index, u64 xcr);
 };
 
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 89af692deb7e..fd7ba820f306 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -4174,11 +4174,27 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 	 * (KVM doesn't change it);
 	 */
 	vcpu->arch.cr0_guest_owned_bits = KVM_POSSIBLE_CR0_GUEST_BITS;
-	vmx_set_cr0(vcpu, vmcs12->host_cr0);
+
+	if (((vmcs12->host_cr0 ^ vcpu->arch.cr0_pinned.one) & vcpu->arch.cr0_pinned.one) ||
+	    ((~vmcs12->host_cr0 ^ vcpu->arch.cr0_pinned.zero) & vcpu->arch.cr0_pinned.zero))
+		nested_vmx_abort(vcpu, VMX_ABORT_VMCS_CORRUPTED);
+
+	vmx_set_cr0(vcpu,
+		    (vmcs12->host_cr0 |
+		     vcpu->arch.cr0_pinned.one) &
+		    ~vcpu->arch.cr0_pinned.zero);
 
 	/* Same as above - no reason to call set_cr4_guest_host_mask().  */
 	vcpu->arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);
-	vmx_set_cr4(vcpu, vmcs12->host_cr4);
+
+	if (((vmcs12->host_cr4 ^ vcpu->arch.cr4_pinned.one) & vcpu->arch.cr4_pinned.one) ||
+	    ((~vmcs12->host_cr4 ^ vcpu->arch.cr4_pinned.zero) & vcpu->arch.cr4_pinned.zero))
+		nested_vmx_abort(vcpu, VMX_ABORT_VMCS_CORRUPTED);
+
+	vmx_set_cr4(vcpu,
+		    (vmcs12->host_cr4 |
+		     vcpu->arch.cr4_pinned.one) &
+		    ~vcpu->arch.cr4_pinned.zero);
 
 	nested_ept_uninit_mmu_context(vcpu);
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 447edc0d1d5a..9851b1263491 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -825,6 +825,11 @@ int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	if ((cr0 & X86_CR0_PG) && !(cr0 & X86_CR0_PE))
 		return 1;
 
+	if (!is_smm(vcpu) && !is_guest_mode(vcpu) &&
+	    (((cr0 ^ vcpu->arch.cr0_pinned.one) & vcpu->arch.cr0_pinned.one) ||
+	    ((~cr0 ^ vcpu->arch.cr0_pinned.zero) & vcpu->arch.cr0_pinned.zero)))
+		return 1;
+
 #ifdef CONFIG_X86_64
 	if ((vcpu->arch.efer & EFER_LME) && !is_paging(vcpu) &&
 	    (cr0 & X86_CR0_PG)) {
@@ -986,6 +991,11 @@ int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	if (kvm_valid_cr4(vcpu, cr4))
 		return 1;
 
+	if (!is_smm(vcpu) && !is_guest_mode(vcpu) &&
+	    (((cr4 ^ vcpu->arch.cr4_pinned.one) & vcpu->arch.cr4_pinned.one) ||
+	    ((~cr4 ^ vcpu->arch.cr4_pinned.zero) & vcpu->arch.cr4_pinned.zero)))
+		return 1;
+
 	if (is_long_mode(vcpu)) {
 		if (!(cr4 & X86_CR4_PAE))
 			return 1;
@@ -1303,6 +1313,12 @@ static const u32 emulated_msrs_all[] = {
 
 	MSR_K7_HWCR,
 	MSR_KVM_POLL_CONTROL,
+	MSR_KVM_CR0_PIN_ALLOWED,
+	MSR_KVM_CR4_PIN_ALLOWED,
+	MSR_KVM_CR0_PINNED_ZERO,
+	MSR_KVM_CR0_PINNED_ONE,
+	MSR_KVM_CR4_PINNED_ZERO,
+	MSR_KVM_CR4_PINNED_ONE,
 };
 
 static u32 emulated_msrs[ARRAY_SIZE(emulated_msrs_all)];
@@ -2995,6 +3011,9 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 	kvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, false);
 }
 
+#define KVM_CR0_PIN_ALLOWED	(X86_CR0_WP)
+#define KVM_CR4_PIN_ALLOWED	(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP | X86_CR4_FSGSBASE)
+
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	bool pr = false;
@@ -3216,6 +3235,66 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		vcpu->arch.msr_kvm_poll_control = data;
 		break;
 
+	case MSR_KVM_CR0_PIN_ALLOWED:
+		if (msr_info->host_initiated) {
+			if (!data)
+				data = KVM_CR0_PIN_ALLOWED;
+			vcpu->arch.cr0_pinned.allowed = data;
+		}
+
+		return (data != vcpu->arch.cr0_pinned.allowed);
+	case MSR_KVM_CR4_PIN_ALLOWED:
+		if (msr_info->host_initiated) {
+			if (!data)
+				data = KVM_CR4_PIN_ALLOWED;
+			vcpu->arch.cr4_pinned.allowed = data;
+		}
+
+		return (data != vcpu->arch.cr4_pinned.allowed);
+	case MSR_KVM_CR0_PINNED_ZERO:
+		if ((data & ~vcpu->arch.cr0_pinned.allowed) ||
+		    (data & vcpu->arch.cr0_pinned.one))
+			return 1;
+
+		if (!msr_info->host_initiated &&
+		    (~data & vcpu->arch.cr0_pinned.zero))
+			return 1;
+
+		vcpu->arch.cr0_pinned.zero = data;
+		break;
+	case MSR_KVM_CR0_PINNED_ONE:
+		if ((data & ~vcpu->arch.cr0_pinned.allowed) ||
+		    (data & vcpu->arch.cr0_pinned.zero))
+			return 1;
+
+		if (!msr_info->host_initiated &&
+		    (~data & vcpu->arch.cr0_pinned.one))
+			return 1;
+
+		vcpu->arch.cr0_pinned.one = data;
+		break;
+	case MSR_KVM_CR4_PINNED_ZERO:
+		if ((data & ~vcpu->arch.cr4_pinned.allowed) ||
+		    (data & vcpu->arch.cr4_pinned.one))
+			return 1;
+
+		if (!msr_info->host_initiated &&
+		    (~data & vcpu->arch.cr4_pinned.zero))
+			return 1;
+
+		vcpu->arch.cr4_pinned.zero = data;
+		break;
+	case MSR_KVM_CR4_PINNED_ONE:
+		if ((data & ~vcpu->arch.cr4_pinned.allowed) ||
+		    (data & vcpu->arch.cr4_pinned.zero))
+			return 1;
+
+		if (!msr_info->host_initiated &&
+		    (~data & vcpu->arch.cr4_pinned.one))
+			return 1;
+
+		vcpu->arch.cr4_pinned.one = data;
+		break;
 	case MSR_IA32_MCG_CTL:
 	case MSR_IA32_MCG_STATUS:
 	case MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:
@@ -3524,6 +3603,24 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 
 		msr_info->data = vcpu->arch.msr_kvm_poll_control;
 		break;
+	case MSR_KVM_CR0_PIN_ALLOWED:
+		msr_info->data = vcpu->arch.cr0_pinned.allowed;
+		break;
+	case MSR_KVM_CR4_PIN_ALLOWED:
+		msr_info->data = vcpu->arch.cr4_pinned.allowed;
+		break;
+	case MSR_KVM_CR0_PINNED_ZERO:
+		msr_info->data = vcpu->arch.cr0_pinned.zero;
+		break;
+	case MSR_KVM_CR0_PINNED_ONE:
+		msr_info->data = vcpu->arch.cr0_pinned.one;
+		break;
+	case MSR_KVM_CR4_PINNED_ZERO:
+		msr_info->data = vcpu->arch.cr4_pinned.zero;
+		break;
+	case MSR_KVM_CR4_PINNED_ONE:
+		msr_info->data = vcpu->arch.cr4_pinned.one;
+		break;
 	case MSR_IA32_P5_MC_ADDR:
 	case MSR_IA32_P5_MC_TYPE:
 	case MSR_IA32_MCG_CAP:
@@ -6830,9 +6927,23 @@ static int emulator_pre_leave_smm(struct x86_emulate_ctxt *ctxt,
 	return kvm_x86_ops.pre_leave_smm(emul_to_vcpu(ctxt), smstate);
 }
 
-static void emulator_post_leave_smm(struct x86_emulate_ctxt *ctxt)
+static int emulator_post_leave_smm(struct x86_emulate_ctxt *ctxt)
 {
-	kvm_smm_changed(emul_to_vcpu(ctxt));
+	struct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);
+	unsigned long cr0 = kvm_read_cr0(vcpu);
+	unsigned long cr4 = kvm_read_cr4(vcpu);
+
+	if (((cr0 ^ vcpu->arch.cr0_pinned.one) & vcpu->arch.cr0_pinned.one) ||
+	    ((~cr0 ^ vcpu->arch.cr0_pinned.zero) & vcpu->arch.cr0_pinned.zero))
+		return 1;
+
+	if (((cr4 ^ vcpu->arch.cr4_pinned.one) & vcpu->arch.cr4_pinned.one) ||
+	    ((~cr4 ^ vcpu->arch.cr4_pinned.zero) & vcpu->arch.cr4_pinned.zero))
+		return 1;
+
+	kvm_smm_changed(vcpu);
+
+	return 0;
 }
 
 static int emulator_set_xcr(struct x86_emulate_ctxt *ctxt, u32 index, u64 xcr)
@@ -10064,6 +10175,11 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 	vcpu->arch.ia32_xss = 0;
 
+	memset(&vcpu->arch.cr0_pinned, 0, sizeof(vcpu->arch.cr0_pinned));
+	vcpu->arch.cr0_pinned.allowed = KVM_CR0_PIN_ALLOWED;
+	memset(&vcpu->arch.cr4_pinned, 0, sizeof(vcpu->arch.cr4_pinned));
+	vcpu->arch.cr4_pinned.allowed = KVM_CR4_PIN_ALLOWED;
+
 	kvm_x86_ops.vcpu_reset(vcpu, init_event);
 }
 
-- 
2.21.0

