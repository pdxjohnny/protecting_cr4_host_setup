From 30fed6e569140c85e4a4f05f374d94509f0765e4 Mon Sep 17 00:00:00 2001
From: John Andersen <john.s.andersen@intel.com>
Date: Tue, 12 Nov 2019 17:28:04 -0500
Subject: [PATCH 2/2] KVM: X86: Add kernel hardening hypercall

Add a kernel hardening hypercall to KVM. Allow guests to request
enabling of protections. Implement CR0 and CR4 bit pinning protections.
A guest may request bits be added (but never removed) from a bit mask
for each register. CR pinning is per kvm, rather than per vcpu. A place
where pinning could be requested on CPU hotplug and after existing
pinning takes place could not be identified. Should the guest attempt to
disable any of the pinned bits which are also currently set in the
control register, send that guest a general protection fault, and leave
the register unchanged. On SMP bringup of non-boot CPUs control
registers will not yet have bits which are pinned set, only send general
protection faults when a write to the control register would unset a bit
which is pinned. When syncing control registers with userspace if
userspace clears bits that are pinned, unpin those bits. QEMU zeros
control registers on reboot, if pinned bits are not cleared initial
writes to control registers by the rebooted guest cause general
protection faults.

Future use of the hardening hypercall may include protecting the NXE bit
of the EFER MSR.

Pinning of sensitive CR bits has already been implemented to protect
against exploits directly calling native_write_crX. The current
protection cannot stop ROP attacks which jump directly to a MOV CR
instruction. Guests running with hypervisor based CR pinning are now
protected against the use of ROP to disable CR bits.

The practice of protecting CRs and MSRs of guests is a known method
implemented by other hypervisors such as HyperV:

https://docs.microsoft.com/en-us/windows-hardware/design/device-experiences/vbs-resource-protections

Guests using the kexec system call currently do not support hypervisor
based control register pinning. This is due to early boot zeroing
protected registers.

A patch for QEMU is required to expose the hardening cpuid feature bit.

https://github.com/qemu/qemu/compare/master...pdxjohnny:cr_pinning.patch

The usage of SMM in SeaBIOS was explored as a way to communicate to KVM
that a reboot has occurred and it should zero the pinned bits. When
using QEMU and SeaBIOS, SMM initialization occurs on reboot. However,
prior to SMM initialization, BIOS writes zero values to CR0, causing a
general protection fault to be sent to the guest before SMM can signal
that the machine has booted.

Signed-off-by: John Andersen <john.s.andersen@intel.com>
---
 Documentation/virt/kvm/hypercalls.txt | 21 ++++++++++
 arch/x86/Kconfig                      | 10 +++++
 arch/x86/include/asm/kvm_host.h       |  2 +
 arch/x86/include/uapi/asm/kvm_para.h  |  1 +
 arch/x86/kernel/cpu/common.c          | 20 ++++++++++
 arch/x86/kvm/cpuid.c                  |  3 +-
 arch/x86/kvm/x86.c                    | 57 +++++++++++++++++++++++++++
 include/uapi/linux/kvm_para.h         |  1 +
 8 files changed, 114 insertions(+), 1 deletion(-)

diff --git a/Documentation/virt/kvm/hypercalls.txt b/Documentation/virt/kvm/hypercalls.txt
index 5f6d291bd004..c9d5b0a3492e 100644
--- a/Documentation/virt/kvm/hypercalls.txt
+++ b/Documentation/virt/kvm/hypercalls.txt
@@ -152,3 +152,24 @@ a0: destination APIC ID
 
 Usage example: When sending a call-function IPI-many to vCPUs, yield if
 any of the IPI target vCPUs was preempted.
+
+12. KVM_HC_CR_PIN
+------------------------
+Architecture: x86
+Status: active
+Purpose: Hypercall used to configure pinned bits in control registers
+Usage:
+
+a0: Control register index
+
+a1: Mask of bits which should be 0 or 1 in the control register.
+
+The hypercall lets a guest enable bit pinning for control registers.
+Instructing the host to enforce guest specified restrictions upon the guest.
+These restrictions cannot be changed for the lifetime of the guest vcpu. This
+is to prevent attackers from disabling them before executing an attack they may
+prevent.
+
+Returns KVM_EINVAL if a1 is 0 or if a1 contains bits which are not allowed to
+be pinned. Bits which are allowed to be pinned are WP for CR0 and SMEP, SMAP,
+and UMIP for CR4.
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 8ef85139553f..f593b90efc34 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -758,6 +758,7 @@ if HYPERVISOR_GUEST
 
 config PARAVIRT
 	bool "Enable paravirtualization code"
+	select PARAVIRT_CR_PIN
 	---help---
 	  This changes the kernel so it can modify itself when it is run
 	  under a hypervisor, potentially improving performance significantly
@@ -839,6 +840,15 @@ config PARAVIRT_TIME_ACCOUNTING
 config PARAVIRT_CLOCK
 	bool
 
+config PARAVIRT_CR_PIN
+	bool "Paravirtual bit pinning for CR0 and CR4"
+	depends on PARAVIRT && !KEXEC
+	help
+	  Select this option to have the virtualised guest request that the
+	  hypervisor disallow it from disabling protections set in control
+	  registers. The hypervisor will prevent exploits from disabling
+	  features such as SMEP, SMAP, UMIP, or WP.
+
 config JAILHOUSE_GUEST
 	bool "Jailhouse non-root cell support"
 	depends on X86_64 && PCI
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index ab8550182a21..6e52f6a2dee9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -545,10 +545,12 @@ struct kvm_vcpu_arch {
 
 	unsigned long cr0;
 	unsigned long cr0_guest_owned_bits;
+	unsigned long cr0_pinned;
 	unsigned long cr2;
 	unsigned long cr3;
 	unsigned long cr4;
 	unsigned long cr4_guest_owned_bits;
+	unsigned long cr4_pinned;
 	unsigned long cr8;
 	u32 pkru;
 	u32 hflags;
diff --git a/arch/x86/include/uapi/asm/kvm_para.h b/arch/x86/include/uapi/asm/kvm_para.h
index 2a8e0b6b9805..bac8354c8e60 100644
--- a/arch/x86/include/uapi/asm/kvm_para.h
+++ b/arch/x86/include/uapi/asm/kvm_para.h
@@ -31,6 +31,7 @@
 #define KVM_FEATURE_PV_SEND_IPI	11
 #define KVM_FEATURE_POLL_CONTROL	12
 #define KVM_FEATURE_PV_SCHED_YIELD	13
+#define KVM_FEATURE_CR_PIN		14
 
 #define KVM_HINTS_REALTIME      0
 
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index fffe21945374..7375f958c185 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -20,6 +20,7 @@
 #include <linux/smp.h>
 #include <linux/io.h>
 #include <linux/syscore_ops.h>
+#include <linux/kvm_para.h>
 
 #include <asm/stackprotector.h>
 #include <asm/perf_event.h>
@@ -423,6 +424,21 @@ void cr4_init(void)
 	this_cpu_write(cpu_tlbstate.cr4, cr4);
 }
 
+static void setup_paravirt_cr_pinning(void)
+{
+#ifdef CONFIG_PARAVIRT_CR_PIN
+	if (kvm_para_has_feature(KVM_FEATURE_CR_PIN)) {
+		if (!kvm_hypercall2(KVM_HC_CR_PIN, 0, X86_CR0_WP))
+			pr_info("Setup paravirtualized cr0 pinning for cpu %d\n",
+					smp_processor_id());
+
+		if (!kvm_hypercall2(KVM_HC_CR_PIN, 4, cr4_pinned_bits))
+			pr_info("Setup paravirtualized cr4 pinning for cpu %d\n",
+					smp_processor_id());
+	}
+#endif
+}
+
 /*
  * Once CPU feature detection is finished (and boot params have been
  * parsed), record any of the sensitive CR bits that are set, and
@@ -435,6 +451,8 @@ static void __init setup_cr_pinning(void)
 	mask = (X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP);
 	cr4_pinned_bits = this_cpu_read(cpu_tlbstate.cr4) & mask;
 	static_key_enable(&cr_pinning.key);
+
+	setup_paravirt_cr_pinning();
 }
 
 /*
@@ -1597,6 +1615,8 @@ void identify_secondary_cpu(struct cpuinfo_x86 *c)
 	mtrr_ap_init();
 	validate_apic_and_package_id(c);
 	x86_spec_ctrl_setup_ap();
+
+	setup_paravirt_cr_pinning();
 }
 
 static __init int setup_noclflush(char *arg)
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index f68c0c753c38..37dd3aa20274 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -712,7 +712,8 @@ static inline int __do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 function,
 			     (1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |
 			     (1 << KVM_FEATURE_PV_SEND_IPI) |
 			     (1 << KVM_FEATURE_POLL_CONTROL) |
-			     (1 << KVM_FEATURE_PV_SCHED_YIELD);
+			     (1 << KVM_FEATURE_PV_SCHED_YIELD) |
+			     (1 << KVM_FEATURE_CR_PIN);
 
 		if (sched_info_on())
 			entry->eax |= (1 << KVM_FEATURE_STEAL_TIME);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 5d530521f11d..445fbd5192de 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -767,6 +767,9 @@ int kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	if ((cr0 & X86_CR0_PG) && !(cr0 & X86_CR0_PE))
 		return 1;
 
+	if ((cr0 ^ old_cr0) & vcpu->arch.cr0_pinned)
+		return 1;
+
 	if (!is_paging(vcpu) && (cr0 & X86_CR0_PG)) {
 #ifdef CONFIG_X86_64
 		if ((vcpu->arch.efer & EFER_LME)) {
@@ -923,6 +926,9 @@ int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	if (kvm_valid_cr4(vcpu, cr4))
 		return 1;
 
+	if ((cr4 ^ old_cr4) & vcpu->arch.cr4_pinned)
+		return 1;
+
 	if (is_long_mode(vcpu)) {
 		if (!(cr4 & X86_CR4_PAE))
 			return 1;
@@ -7374,6 +7380,44 @@ static void kvm_sched_yield(struct kvm *kvm, unsigned long dest_id)
 		kvm_vcpu_yield_to(target);
 }
 
+#define KVM_CR0_PIN_ALLOWED	X86_CR0_WP
+#define KVM_CR4_PIN_ALLOWED	X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP
+
+static unsigned long kvm_pin_cr_bits(struct kvm_vcpu *vcpu,
+				     unsigned long cr,
+				     unsigned long pin)
+{
+	unsigned long ret;
+
+	switch (cr) {
+	case 0:
+		if (vcpu->arch.cr0_pinned || (pin & ~KVM_CR0_PIN_ALLOWED)) {
+			ret = -KVM_EINVAL;
+			break;
+		}
+		vcpu->arch.cr0_pinned |= pin;
+		kvm_x86_ops->set_cr0_guest_owned_bits(vcpu,
+				vcpu->arch.cr0_guest_owned_bits & ~pin);
+		ret = 0;
+		break;
+	case 4:
+		if (vcpu->arch.cr4_pinned || (pin & ~KVM_CR4_PIN_ALLOWED)) {
+			ret = -KVM_EINVAL;
+			break;
+		}
+		vcpu->arch.cr4_pinned |= pin;
+		kvm_x86_ops->set_cr4_guest_owned_bits(vcpu,
+				vcpu->arch.cr4_guest_owned_bits & ~pin);
+		ret = 0;
+		break;
+	default:
+		ret = -KVM_EOPNOTSUPP;
+		break;
+	}
+
+	return ret;
+}
+
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 {
 	unsigned long nr, a0, a1, a2, a3, ret;
@@ -7425,6 +7469,9 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 		kvm_sched_yield(vcpu->kvm, a0);
 		ret = 0;
 		break;
+	case KVM_HC_CR_PIN:
+		ret = kvm_pin_cr_bits(vcpu, a0, a1);
+		break;
 	default:
 		ret = -KVM_ENOSYS;
 		break;
@@ -8802,10 +8849,20 @@ static int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)
 	mmu_reset_needed |= vcpu->arch.efer != sregs->efer;
 	kvm_x86_ops->set_efer(vcpu, sregs->efer);
 
+	vcpu->arch.cr0_pinned &= sregs->cr0;
+	kvm_x86_ops->set_cr0_guest_owned_bits(vcpu,
+			vcpu->arch.cr0_guest_owned_bits &
+			~vcpu->arch.cr0_pinned);
+
 	mmu_reset_needed |= kvm_read_cr0(vcpu) != sregs->cr0;
 	kvm_x86_ops->set_cr0(vcpu, sregs->cr0);
 	vcpu->arch.cr0 = sregs->cr0;
 
+	vcpu->arch.cr4_pinned &= sregs->cr4;
+	kvm_x86_ops->set_cr4_guest_owned_bits(vcpu,
+			vcpu->arch.cr4_guest_owned_bits &
+			~vcpu->arch.cr4_pinned);
+
 	mmu_reset_needed |= kvm_read_cr4(vcpu) != sregs->cr4;
 	cpuid_update_needed |= ((kvm_read_cr4(vcpu) ^ sregs->cr4) &
 				(X86_CR4_OSXSAVE | X86_CR4_PKE));
diff --git a/include/uapi/linux/kvm_para.h b/include/uapi/linux/kvm_para.h
index 8b86609849b9..20916c70d8f2 100644
--- a/include/uapi/linux/kvm_para.h
+++ b/include/uapi/linux/kvm_para.h
@@ -29,6 +29,7 @@
 #define KVM_HC_CLOCK_PAIRING		9
 #define KVM_HC_SEND_IPI		10
 #define KVM_HC_SCHED_YIELD		11
+#define KVM_HC_CR_PIN			12
 
 /*
  * hypercalls use architecture specific
-- 
2.21.0

