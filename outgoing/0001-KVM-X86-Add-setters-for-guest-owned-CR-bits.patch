From 538849f08862a63db632849f05ce052aee99773f Mon Sep 17 00:00:00 2001
From: John Andersen <john.s.andersen@intel.com>
Date: Fri, 15 Nov 2019 13:25:37 -0500
Subject: [PATCH 1/2] KVM: X86: Add setters for guest owned CR bits

Ensure intercepts are always kept in sync by adding function pointers to
kvm_x86_ops to set guest owned bits for CR0 and CR4. In VMX, set VMCS
host guest mask to the opposite of the guest owned bits. In SVM
check guest owned bits and set VMCB to intercept writes if any bits are
owned by the host.

Signed-off-by: John Andersen <john.s.andersen@intel.com>
---
 arch/x86/include/asm/kvm_host.h |  4 ++++
 arch/x86/kvm/svm.c              | 40 ++++++++++++++++++++++++++++++++-
 arch/x86/kvm/vmx/nested.c       |  8 +++----
 arch/x86/kvm/vmx/vmx.c          | 40 +++++++++++++++++++--------------
 arch/x86/kvm/vmx/vmx.h          |  8 +++++++
 5 files changed, 78 insertions(+), 22 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 4fc61483919a..ab8550182a21 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1046,6 +1046,10 @@ struct kvm_x86_ops {
 	void (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);
 	void (*set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);
 	int (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);
+	void (*set_cr0_guest_owned_bits)(struct kvm_vcpu *vcpu,
+			unsigned long cr0_guest_owned_bits);
+	void (*set_cr4_guest_owned_bits)(struct kvm_vcpu *vcpu,
+			unsigned long cr4_guest_owned_bits);
 	void (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);
 	void (*get_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
 	void (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index c5673bda4b66..ac1cd7773df1 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -2541,7 +2541,7 @@ static void update_cr0_intercept(struct vcpu_svm *svm)
 
 	mark_dirty(svm->vmcb, VMCB_CR);
 
-	if (gcr0 == *hcr0) {
+	if (gcr0 == *hcr0 && ~svm->vcpu.arch.cr0_guest_owned_bits == 0U) {
 		clr_cr_intercept(svm, INTERCEPT_CR0_READ);
 		clr_cr_intercept(svm, INTERCEPT_CR0_WRITE);
 	} else {
@@ -2604,6 +2604,42 @@ static int svm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	return 0;
 }
 
+static void svm_set_cr0_guest_owned_bits(struct kvm_vcpu *vcpu,
+					 unsigned long cr0_guest_owned_bits)
+{
+	struct vcpu_svm *svm = to_svm(vcpu);
+
+	mark_dirty(svm->vmcb, VMCB_CR);
+
+	if (~cr0_guest_owned_bits == 0U) {
+		set_cr_intercept(svm, INTERCEPT_CR0_READ);
+		set_cr_intercept(svm, INTERCEPT_CR0_WRITE);
+	} else {
+		clr_cr_intercept(svm, INTERCEPT_CR0_READ);
+		clr_cr_intercept(svm, INTERCEPT_CR0_WRITE);
+	}
+
+	vcpu->arch.cr0_guest_owned_bits = cr0_guest_owned_bits;
+}
+
+static void svm_set_cr4_guest_owned_bits(struct kvm_vcpu *vcpu,
+					 unsigned long cr4_guest_owned_bits)
+{
+	struct vcpu_svm *svm = to_svm(vcpu);
+
+	mark_dirty(svm->vmcb, VMCB_CR);
+
+	if (~cr4_guest_owned_bits == 0U) {
+		set_cr_intercept(svm, INTERCEPT_CR4_READ);
+		set_cr_intercept(svm, INTERCEPT_CR4_WRITE);
+	} else {
+		clr_cr_intercept(svm, INTERCEPT_CR4_READ);
+		clr_cr_intercept(svm, INTERCEPT_CR4_WRITE);
+	}
+
+	vcpu->arch.cr4_guest_owned_bits = cr4_guest_owned_bits;
+}
+
 static void svm_set_segment(struct kvm_vcpu *vcpu,
 			    struct kvm_segment *var, int seg)
 {
@@ -7219,6 +7255,8 @@ static struct kvm_x86_ops svm_x86_ops __ro_after_init = {
 	.set_cr0 = svm_set_cr0,
 	.set_cr3 = svm_set_cr3,
 	.set_cr4 = svm_set_cr4,
+	.set_cr0_guest_owned_bits = svm_set_cr0_guest_owned_bits,
+	.set_cr4_guest_owned_bits = svm_set_cr4_guest_owned_bits,
 	.set_efer = svm_set_efer,
 	.get_idt = svm_get_idt,
 	.set_idt = svm_set_idt,
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index 0e7c9301fe86..fac5c5994548 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -2288,7 +2288,7 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
 	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);
 	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);
 
-	set_cr4_guest_host_mask(vmx);
+	vmx_set_cr4_guest_owned_bits(&vmx->vcpu, KVM_CR4_GUEST_OWNED_BITS);
 }
 
 /*
@@ -2336,8 +2336,8 @@ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 	 * trap. Note that CR0.TS also needs updating - we do this later.
 	 */
 	update_exception_bitmap(vcpu);
-	vcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;
-	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);
+	vmx_set_cr0_guest_owned_bits(vcpu, vcpu->arch.cr0_guest_owned_bits &
+			~vmcs12->cr0_guest_host_mask);
 
 	if (vmx->nested.nested_run_pending &&
 	    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_PAT)) {
@@ -3812,7 +3812,7 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,
 	vcpu->arch.cr0_guest_owned_bits = X86_CR0_TS;
 	vmx_set_cr0(vcpu, vmcs12->host_cr0);
 
-	/* Same as above - no reason to call set_cr4_guest_host_mask().  */
+	/* Same as above - no reason to call vmx_set_cr4_guest_owned_bits().  */
 	vcpu->arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);
 	vmx_set_cr4(vcpu, vmcs12->host_cr4);
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 04a8212704c1..017661898268 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -131,9 +131,6 @@ module_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);
 #define KVM_VM_CR0_ALWAYS_ON				\
 	(KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST | 	\
 	 X86_CR0_WP | X86_CR0_PG | X86_CR0_PE)
-#define KVM_CR4_GUEST_OWNED_BITS				      \
-	(X86_CR4_PVI | X86_CR4_DE | X86_CR4_PCE | X86_CR4_OSFXSR      \
-	 | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_TSD)
 
 #define KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR4_VMXE
 #define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)
@@ -3097,6 +3094,25 @@ int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)
 	return 0;
 }
 
+void vmx_set_cr0_guest_owned_bits(struct kvm_vcpu *vcpu,
+				  unsigned long cr0_guest_owned_bits)
+{
+	vcpu->arch.cr0_guest_owned_bits = cr0_guest_owned_bits;
+	vmcs_writel(CR0_GUEST_HOST_MASK, ~cr0_guest_owned_bits);
+}
+
+void vmx_set_cr4_guest_owned_bits(struct kvm_vcpu *vcpu,
+				  unsigned long cr4_guest_owned_bits)
+{
+	vcpu->arch.cr4_guest_owned_bits = cr4_guest_owned_bits;
+	if (enable_ept)
+		vcpu->arch.cr4_guest_owned_bits |= X86_CR4_PGE;
+	if (is_guest_mode(vcpu))
+		vcpu->arch.cr4_guest_owned_bits &=
+			~get_vmcs12(vcpu)->cr4_guest_host_mask;
+	vmcs_writel(CR4_GUEST_HOST_MASK, ~vcpu->arch.cr4_guest_owned_bits);
+}
+
 void vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -3927,17 +3943,6 @@ void vmx_set_constant_host_state(struct vcpu_vmx *vmx)
 		vmcs_write64(HOST_IA32_EFER, host_efer);
 }
 
-void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)
-{
-	vmx->vcpu.arch.cr4_guest_owned_bits = KVM_CR4_GUEST_OWNED_BITS;
-	if (enable_ept)
-		vmx->vcpu.arch.cr4_guest_owned_bits |= X86_CR4_PGE;
-	if (is_guest_mode(&vmx->vcpu))
-		vmx->vcpu.arch.cr4_guest_owned_bits &=
-			~get_vmcs12(&vmx->vcpu)->cr4_guest_host_mask;
-	vmcs_writel(CR4_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr4_guest_owned_bits);
-}
-
 u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)
 {
 	u32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;
@@ -4243,10 +4248,9 @@ static void vmx_vcpu_setup(struct vcpu_vmx *vmx)
 	/* 22.2.1, 20.8.1 */
 	vm_entry_controls_set(vmx, vmx_vmentry_ctrl());
 
-	vmx->vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;
-	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);
+	vmx_set_cr0_guest_owned_bits(&vmx->vcpu, X86_CR0_TS);
 
-	set_cr4_guest_host_mask(vmx);
+	vmx_set_cr4_guest_owned_bits(&vmx->vcpu, KVM_CR4_GUEST_OWNED_BITS);
 
 	if (vmx_xsaves_supported())
 		vmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);
@@ -7786,6 +7790,8 @@ static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
 	.set_cr0 = vmx_set_cr0,
 	.set_cr3 = vmx_set_cr3,
 	.set_cr4 = vmx_set_cr4,
+	.set_cr0_guest_owned_bits = vmx_set_cr0_guest_owned_bits,
+	.set_cr4_guest_owned_bits = vmx_set_cr4_guest_owned_bits,
 	.set_efer = vmx_set_efer,
 	.get_idt = vmx_get_idt,
 	.set_idt = vmx_set_idt,
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 5a0f34b1e226..79c9b5b66b6a 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -321,6 +321,10 @@ void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer);
 void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);
 void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);
 int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);
+void vmx_set_cr0_guest_owned_bits(struct kvm_vcpu *vcpu,
+				  unsigned long cr0_guest_owned_bits);
+void vmx_set_cr4_guest_owned_bits(struct kvm_vcpu *vcpu,
+				  unsigned long cr4_guest_owned_bits);
 void set_cr4_guest_host_mask(struct vcpu_vmx *vmx);
 void ept_save_pdptrs(struct kvm_vcpu *vcpu);
 void vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);
@@ -335,6 +339,10 @@ struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr);
 void pt_update_intercept_for_msr(struct vcpu_vmx *vmx);
 void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp);
 
+#define KVM_CR4_GUEST_OWNED_BITS				      \
+	(X86_CR4_PVI | X86_CR4_DE | X86_CR4_PCE | X86_CR4_OSFXSR      \
+	 | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_TSD)
+
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1
 
-- 
2.21.0

